#!/bin/bash
set -e

export PROJECT="mozaggregator2bq"
export DATASET="aggregates"
export DATA_DIR="data"
LOAD_BQ=${LOAD_BQ:-false}

function to_ds {
    DS_NODASH=$1 python3 - <<EOD
from datetime import datetime
from os import environ
ds = environ["DS_NODASH"]
print(datetime.strptime(ds, "%Y%m%d").strftime("%Y-%m-%d"))
EOD
}

function partition_exists {
    local table=$1
    local partition=$2
    local result
    result=$(bq query \
        --nouse_legacy_sql \
        --format json \
        "SELECT
            COUNT(1) as count
        FROM
            $table
        WHERE
            DATE(_PARTITIONTIME) = '$(to_ds "$partition")'" | \
            jq -r ".[0].count"
    )
    ((result > 0))
}

table="aggregates.submission_date_aggregates"
partition=20200101
partition_exists $table $partition

function run_day {
    local aggregate_type=$1
    local ds_nodash=$2
    
    local input="$DATA_DIR/$aggregate_type/$ds_nodash"
    local intermediate="$DATA_DIR/parquet/$aggregate_type/$ds_nodash"
    local output=gs://$PROJECT/$DATA_DIR/$aggregate_type/$ds_nodash
    
    # check if this has already been done
    if ! gsutil stat "$output/_SUCCESS"; then
        # dump the table
        AGGREGATE_TYPE=$aggregate_type \
        DS_NODASH=$ds_nodash \
            scripts/pg_dump_by_day

        # create parquet
        echo "running for $intermediate"
        bin/submit-local scripts/pg_dump_to_parquet.py \
            --input-dir "$input" \
            --output-dir "$intermediate"
        
        gsutil rsync -d -r "$intermediate/" "$output/"
    fi
    local table="$DATASET.${aggregate_type}_aggregates"
    if ! $LOAD_BQ; then
        echo "skipping bq load for $table"
        return
    fi
    if ! partition_exists "$PROJECT.$table" "$ds_nodash"; then
        echo "loading partition $ds_nodash into $table"
        bq load \
            --source_format=PARQUET \
            --autodetect \
            --replace \
            --time_partitioning_type DAY \
            --clustering_fields metric,version,channel,os \
            "$PROJECT:${table}\$${ds_nodash}" \
            "$output/*.parquet"
    else
        echo "partition $ds_nodash already exists in $table"
    fi
}


function ds_nodash_range {
    DS_START=$1 DS_END=$2 python3 - <<EOD
from datetime import date, timedelta, datetime
from os import environ

def parse(ds):
    return datetime.strptime(ds, "%Y-%m-%d")

start_date = parse(environ["DS_START"])
end_date = parse(environ["DS_END"])

dates = []
for i in range((end_date - start_date).days):
    dt = start_date + timedelta(i)
    dates.append(dt.strftime("%Y%m%d"))
print("\n".join(dates))
EOD
}


cd "$(dirname "$0")/.."

# checking if spark is enabled
python -c "import pyspark; print(pyspark.__path__[0])"

# checking if credentials are set, check export_credentials_s3 for full list
: "${POSTGRES_USER?}"

original_project=$(gcloud config get-value project)
function cleanup {
    gcloud config set project "$original_project"
}
trap cleanup EXIT
gcloud config set project $PROJECT

if ! bq ls $PROJECT:$DATASET; then
    bq mk $PROJECT:$DATASET
fi


mkdir -p "$DATA_DIR"
start_ds="2020-01-01"
end_ds="2020-01-08"
for ds_nodash in $(ds_nodash_range "$start_ds" "$end_ds"); do
    time run_day "submission_date" "$ds_nodash"
    time run_day "build_id" "$ds_nodash"
done
